#   WS_median = median,
#   WS_min = min,
#   WS_max = max,
#   WS_sd = sd,
#   WS_var = var,
#   WS_sum = sum,
#   WS_skew = moments::skewness,
#   WS_kurt = moments::kurtosis
# )
# WS_BR = stack("/path/to/u2_20010101_20240320_BR-DWGD_UFES_UTEXAS_v_3.2.3.nc")
#
# layer_start_2023 <- 8036
# layer_end_2023 <- 8400
# WS_BR_2023 <- WS_BR[[layer_start_2023:layer_end_2023]]
#
# WS_borders_RO_projected <- st_transform(borders_RO, crs(WS_BR_2023))
# WS_borders_RO_sp <- as(WS_borders_RO_projected, "Spatial")
#
# # first we crop a retangular box, then we adjust it
# WS_RO_2023 = crop(WS_BR_2023, WS_borders_RO_sp)
# WS_RO_2023 <- mask(WS_RO_2023, WS_borders_RO_sp)
#
# # lets save it
# writeRaster(WS_RO_2023, filename = "maps/WS_RO_2023.nc", format = "CDF", overwrite = TRUE)
# WS_RO_2023 = stack("maps/WS_RO_2023.nc")
#
# WS_RO_stats <- suppressWarnings(lapply(names(stat_functions_WS), function(name) {
#   fun_to_apply <- stat_functions_WS[[name]]
#   stat_raster <- raster::calc(WS_RO_2023, fun=fun_to_apply, na.rm=TRUE)
#   names(stat_raster) <- name
#   return(stat_raster)
# }))
# # suppressing warnings
# fixing for sum = 0
# WS_RO_stats$WS_sum[WS_RO_stats$WS_sum == 0] = NA
#
#
# WS_RO_stats <- raster::stack(WS_RO_stats)
# WS_RO_stats <- rasterToPolygons(WS_RO_stats)
# WS_RO_stats <- st_as_sf(WS_RO_stats)
# WS_RO_stats <- st_transform(WS_RO_stats, st_crs(borders_RO))
# WS_RO_stats <- st_intersection(WS_RO_stats, borders_RO)
#
# saveRDS(WS_RO_stats, "maps/WS_RO_2023_stats.rds")
WS_RO_stats = readRDS("maps/WS_RO_2023_stats.rds")
plot_map_RO(WS_RO_stats, "WS_mean",
map_legend_title = "Mean wind speed (m/s)",
"WS_RO_mean")
plot_map_RO(WS_RO_stats, "WS_median",
map_legend_title = "Median wind speed (m/s)",
"WS_RO_median")
plot_map_RO(WS_RO_stats, "WS_min",
map_legend_title = "Minimum wind speed (m/s)",
"WS_RO_min")
plot_map_RO(WS_RO_stats, "WS_max",
map_legend_title = "Maximum wind speed (m/s)",
"WS_RO_max")
plot_map_RO(WS_RO_stats, "WS_sd",
map_legend_title = "Standard deviation wind speed (m/s)",
"WS_RO_sd")
plot_map_RO(WS_RO_stats, "WS_var",
map_legend_title = "Variance wind speed",
"WS_RO_var")
plot_map_RO(WS_RO_stats, "WS_sum",
map_legend_title = "Sum wind speed (m/s)",
"WS_RO_sum")
plot_map_RO(WS_RO_stats, "WS_skew",
map_legend_title = "Skewness wind speed",
"WS_RO_skew")
plot_map_RO(WS_RO_stats, "WS_kurt",
map_legend_title = "Kurtosis wind speed",
"WS_RO_kurt")
data_cleanest$WS_mean = solar_irradiation_by_coordinates(coords_all,
WS_RO_stats, variable_name = "WS_mean")
data_cleanest$WS_median = solar_irradiation_by_coordinates(coords_all,
WS_RO_stats, variable_name = "WS_median")
data_cleanest$WS_min = solar_irradiation_by_coordinates(coords_all,
WS_RO_stats, variable_name = "WS_min")
data_cleanest$WS_max = solar_irradiation_by_coordinates(coords_all,
WS_RO_stats, variable_name = "WS_max")
data_cleanest$WS_sd = solar_irradiation_by_coordinates(coords_all,
WS_RO_stats, variable_name = "WS_sd")
data_cleanest$WS_var = solar_irradiation_by_coordinates(coords_all,
WS_RO_stats, variable_name = "WS_var")
data_cleanest$WS_sum = solar_irradiation_by_coordinates(coords_all,
WS_RO_stats, variable_name = "WS_sum")
data_cleanest$WS_skew = solar_irradiation_by_coordinates(coords_all,
WS_RO_stats, variable_name = "WS_skew")
data_cleanest$WS_kurt = solar_irradiation_by_coordinates(coords_all,
WS_RO_stats, variable_name = "WS_kurt")
plot_scaterplot_points_Yf_PRGHI_CUF(data_cleanest, "WS_mean", "Mean wind speed (m/s)")
plot_scaterplot_points_Yf_PRGHI_CUF(data_cleanest, "WS_median", "Median wind speed (m/s)")
plot_scaterplot_points_Yf_PRGHI_CUF(data_cleanest, "WS_min", "Minimum wind speed (m/s)")
plot_scaterplot_points_Yf_PRGHI_CUF(data_cleanest, "WS_max", "Maximum wind speed (m/s)")
plot_scaterplot_points_Yf_PRGHI_CUF(data_cleanest, "WS_sd", "Standard deviation wind speed (m/s)")
plot_scaterplot_points_Yf_PRGHI_CUF(data_cleanest, "WS_var", "Variance wind speed")
plot_scaterplot_points_Yf_PRGHI_CUF(data_cleanest, "WS_sum", "Sum wind speed (m/s)")
plot_scaterplot_points_Yf_PRGHI_CUF(data_cleanest, "WS_skew", "Skewness wind speed")
plot_scaterplot_points_Yf_PRGHI_CUF(data_cleanest, "WS_kurt", "Kurtosis wind speed")
model.WS = list()
model.WS[[1]] = gam(PR_GHI ~ s(WS_mean), data = data_cleanest)
model.WS[[2]] = gam(PR_GHI ~ s(WS_median, k = 5), data = data_cleanest)
model.WS[[3]] = gam(PR_GHI ~ s(WS_min, k = 3), data = data_cleanest)
model.WS[[4]] = gam(PR_GHI ~ s(WS_max, k = 7), data = data_cleanest)
model.WS[[5]] = gam(PR_GHI ~ s(WS_sd), data = data_cleanest)
model.WS[[6]] = gam(PR_GHI ~ s(WS_var), data = data_cleanest)
model.WS[[7]] = gam(PR_GHI ~ s(WS_sum), data = data_cleanest)
model.WS[[8]] = gam(PR_GHI ~ s(WS_skew), data = data_cleanest)
model.WS[[9]] = gam(PR_GHI ~ s(WS_kurt), data = data_cleanest)
# Lets calculate p-values
p_value.WS = 0
p_value.WS[1] = anova(best_model.PR_GHI, model.WS[[1]])$"Pr(>F)"[2]
p_value.WS[2] = anova(best_model.PR_GHI, model.WS[[2]])$"Pr(>F)"[2]
p_value.WS[3] = anova(best_model.PR_GHI, model.WS[[3]])$"Pr(>F)"[2]
p_value.WS[4] = anova(best_model.PR_GHI, model.WS[[4]])$"Pr(>F)"[2]
p_value.WS[5] = anova(best_model.PR_GHI, model.WS[[5]])$"Pr(>F)"[2]
p_value.WS[6] = anova(best_model.PR_GHI, model.WS[[6]])$"Pr(>F)"[2]
p_value.WS[7] = anova(best_model.PR_GHI, model.WS[[7]])$"Pr(>F)"[2]
p_value.WS[8] = anova(best_model.PR_GHI, model.WS[[8]])$"Pr(>F)"[2]
p_value.WS[9] = anova(best_model.PR_GHI, model.WS[[9]])$"Pr(>F)"[2]
# Since we are performing several test, our calculated p-value is inflated
# We need to correct this p-value. We will use the False Discovery Rate
# which is not as strict as other methodologies (e.g., Bonferroni correction)
# Note: on reality, we should correct the p-value of all tests. That would result
# in a larger p-value. Since we are just exploring, what we are doing is enough
adjusted_p_values.WS = p.adjust(p_value.WS, method = "BH")
# 0.486025 0.486025 0.486025 0.486025 0.486025 0.486025 0.486025 0.486025 0.486025
# After adjusting p-values, we see that we do not have anything that is statistically significant
# Lets also take a look on AIC
AIC_statistical_models.WS = c(AIC(best_model.PR_GHI), sapply(model.WS, AIC))
# -301.1569 -299.8295 -299.6498 -302.1608 -299.5889 -299.6243 -299.3083 -299.8295 -300.2296 -299.8119
# We could say that maybe WS_max has an effect, because it has lower AIC. Lets investiage
summary(model.WS[[4]]) # nothing really statistically significant
plot(model.WS[[4]]) # WS_max plot includes zero
gam.check(model.WS[[4]])
# lets fit again considering a line
model.WS.4 = gam(PR_GHI ~ WS_max, data = data_cleanest)
summary(model.WS.4) # nothing is significant anymore
# not great
# best model is still the previous model
name_predictors_WS = c("Null model",
"Median WS", "Minimum WS",
"Maximum WS", "SD WS",
"Variance WS", "Sum WS",
"Skewness WS", "Kurtosis WS")
plot_p_values_and_AIC(c(NA, adjusted_p_values.WS), AIC_statistical_models.WS,
name_predictors_WS, "WS_RO_p_values")
# saving the dataset with all datapoints
write_csv(data_cleanest, "data_cleanest.csv")
# lets take another look in our dataset
# lets look at correlation
data_numeric <- data_cleanest %>%
dplyr::select(where(is.numeric)) %>%
na.omit()
# removing near zero correlations
zero_sd_cols <- sapply(data_numeric, sd) %>%
{ is.na(.) | . < 1e-6 } %>%
which() %>%
names()
if (length(zero_sd_cols) > 0) {
message(paste("Removing near zero standard deviation columns:", paste(zero_sd_cols, collapse = ", ")))
data_numeric <- data_numeric %>%
dplyr::select(-all_of(zero_sd_cols))
} else {
data_numeric <- data_numeric
}
# removing final_yield_estimated
data_numeric <- data_numeric %>%
dplyr::select(-final_yield_estimated)
names_data_numeric = c(
"plain('PV mod power')", "plain('PV mod qnt')", "plain('PV inv power')", "plain('PV inv qnt')",
"plain('Yearly kWh')", "plain('PV inv tot power')", "plain('DC kWp')", "plain('ISF')",
"plain('Temp Coeff ') * I[sc]", "plain('Temp Coeff ') * V[oc]", "plain('Temp Coeff ') * P[max]",
"plain('PV mod eff')", "plain('PV inv max eff')", "plain('PV inv Eur eff')",
"plain('PV inv #MPPTs')", "plain('PV inv #DC inputs')", "plain('#DC inputs/#MPPTs')",
"plain('Latitude')", "plain('Longitude')","plain('Diffuse')", "plain('Direct normal')",
"plain('Global horizontal')", "plain('Tilted latitude')", "plain('Final yield')",
"plain('Hours peak sun')", "PR[GHI]", "plain('CUF')",
"plain('Total #MPPTs')", "plain('Total #DC inputs')",
## Tmax
"plain('Mean ') * T[max]", "plain('Median ') * T[max]", "plain('Minimum ') * T[max]",
"plain('Maximum ') * T[max]", "plain('SD ') * T[max]", "plain('Variance ') * T[max]",
"plain('Sum ') * T[max]", "plain('Skewness ') * T[max]", "plain('Kurtosis ') * T[max]",
## Tmin
"plain('Mean ') * T[min]", "plain('Median ') * T[min]", "plain('Minimum ') * T[min]",
"plain('Maximum ') * T[min]", "plain('SD ') * T[min]", "plain('Variance ') * T[min]",
"plain('Sum ') * T[min]", "plain('Skewness') * T[min]", "plain('Kurtosis') * T[min]",
## Precipitation
"plain('Mean prec')", "plain('Median prec')",
"plain('Maximum prec')", "plain('SD prec')", "plain('Variance prec')",
"plain('Sum prec')", "plain('Skewness prec')", "plain('Kurtosis prec')",
## Relative humidity
"plain('Mean RH')", "plain('Median RH')", "plain('Minimum RH')",
"plain('Maximum RH')", "plain('SD RH')", "plain('Variance RH')",
"plain('Sum RH')", "plain('Skewness RH')", "plain('Kurtosis RH')",
## Wind speed
"plain('Mean WS')", "plain('Median WS')", "plain('Minimum WS')",
"plain('Maximum WS')", "plain('SD WS')", "plain('Variance WS')",
"plain('Sum WS')", "plain('Skewness WS')", "plain('Kurtosis WS')"
)
plot_correlation = function (data_numeric.input, variable_to_calculate,
name_of_variable = "", name_to_save = "",
corr.method = "pearson", number_to_remove = 1) {
if (name_to_save == "") {
show_not_save = TRUE
} else {
show_not_save = FALSE
png(filename = paste0("plots/",name_to_save, ".png"), width = 366, height = 1100, units = "px")
}
cor_matrix <- cor(data_numeric.input, method = corr.method)
variable_to_plot = cor_matrix[,variable_to_calculate]
names(variable_to_plot) = names_data_numeric
variable_to_plot <- variable_to_plot %>%
sort(decreasing = TRUE)
# removing correlation with itself
variable_to_plot = variable_to_plot[(1 + number_to_remove):length(variable_to_plot)]
# data conversion to use ggplot
cor_df_long <- data.frame(
Variable = names(variable_to_plot),
Correlation = variable_to_plot
)
if (corr.method == "pearson") {
corelation_name = "correlation (r)"
} else if (corr.method == "spearman") {
corelation_name = "association (Ï)"
} else if (corr.method == "kendall") {
corelation_name = "association ($\tau$)"
}
plot_of_correlations = ggplot(cor_df_long, aes(x = reorder(Variable, Correlation), y = Correlation)) +
geom_col(aes(fill = Correlation), width = 0.8, color = "black", linewidth = 0.1) +
geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
scale_fill_gradient2(low = "darkred", mid = "white", high = "darkblue",
midpoint = 0, limit = c(-1, 1),
name = paste(name_of_variable,corelation_name)) +
guides(fill = guide_colorbar(
title.position = "top",
barwidth = unit(0.7, "npc"),
barheight = 1.5,
)) +
labs(x = NULL, y = NULL) +
coord_flip() +
scale_x_discrete(labels = scales::parse_format()) +
theme_minimal() +
theme(
legend.position = "bottom",
legend.title = element_text(size = 14, hjust = 0.5),
legend.box.margin = margin(t = -10, r = 0, b = 0, l = -75, unit = "pt"),
axis.title = element_blank(),
axis.text.y = element_text(size = 14),
axis.text.x = element_text(angle = 90, hjust = 1, size = 14),
plot.title = element_text(size = 14)
)
print(plot_of_correlations)
if (!show_not_save) {
dev.off()
}
}
plot_correlation(data_numeric, "PR_GHI", "Performance ratio at GHI", "PR_GHI_corr", "spearman")
plot_correlation(data_numeric, "global_horizontal", "Global horizontal irradiance", "GHI_corr", "spearman")
plot_correlation(data_numeric, "final_yield", "Final yield", "final_yield_corr", "spearman")
plot_correlation(data_numeric, "CUF", "Capacity utilization factor", "CUF_corr", "spearman")
# note that correlation with CUF/ISF will be the same as correlation with final yield
############ What is our statistical power?
# So, we found nothing. Could it be that we did not have enought statistical power? What is the smallest effect we could have detected?
n.pwr = 142
k.pwr = 1
v_denom.pwr = n.pwr - k.pwr - 1
alpha.pwr = 0.05
power.pwr = 0.8
sensitivity_analysis <- pwr.f2.test(
u = k.pwr,
v = v_denom.pwr,
f2 = NULL,  # Set to NULL to solve for effect size
sig.level = alpha.pwr,
power = power.pwr
)
print(sensitivity_analysis)
R2.pwr = sensitivity_analysis$f2/(1+sensitivity_analysis$f2)
range.95.PI = PR_GUI_boot_all_percentile_and_quantiles[11] - PR_GUI_boot_all_percentile_and_quantiles[10]
reduction_in_uncertainty = 1 - sqrt(1 - R2.pwr)
could.explain.at.least = range.95.PI*reduction_in_uncertainty
######### now we will calculate what n we would need to detect any significant change on PRGHI
could.explain.at.least_to_calculate_n = 0.1*0.3
reduction_in_uncertainty_to_calculate_n = could.explain.at.least_to_calculate_n/range.95.PI
R2.pwr_to_calculate_n = 1 - (1 - reduction_in_uncertainty_to_calculate_n)^2
f2_to_calculate_n = R2.pwr_to_calculate_n/(1-R2.pwr_to_calculate_n)
sensitivity_analysis_to_calculate_n <- pwr.f2.test(
u = k.pwr,
v = NULL,
f2 = f2_to_calculate_n,  # Set to NULL to solve for effect size
sig.level = alpha.pwr,
power = power.pwr
)
print(sensitivity_analysis_to_calculate_n)
n_to_calcualte_n = ceiling(sensitivity_analysis_to_calculate_n$v + k.pwr + 1)
############ What is our statistical power?
# So, we found nothing. Could it be that we did not have enought statistical power? What is the smallest effect we could have detected?
n.pwr = 142
k.pwr = 1
v_denom.pwr = n.pwr - k.pwr - 1
alpha.pwr = 0.05
power.pwr = 0.8
sensitivity_analysis <- pwr.f2.test(
u = k.pwr,
v = v_denom.pwr,
f2 = NULL,  # Set to NULL to solve for effect size
sig.level = alpha.pwr,
power = power.pwr
)
print(sensitivity_analysis)
R2.pwr = sensitivity_analysis$f2/(1+sensitivity_analysis$f2)
range.95.PI = PR_GUI_boot_all_percentile_and_quantiles[11] - PR_GUI_boot_all_percentile_and_quantiles[10]
reduction_in_uncertainty = 1 - sqrt(1 - R2.pwr)
could.explain.at.least = range.95.PI*reduction_in_uncertainty
######### now we will calculate what n we would need to detect any significant change on PRGHI
could.explain.at.least_to_calculate_n = 0.1*0.3
reduction_in_uncertainty_to_calculate_n = could.explain.at.least_to_calculate_n/range.95.PI
R2.pwr_to_calculate_n = 1 - (1 - reduction_in_uncertainty_to_calculate_n)^2
f2_to_calculate_n = R2.pwr_to_calculate_n/(1-R2.pwr_to_calculate_n)
sensitivity_analysis_to_calculate_n <- pwr.f2.test(
u = k.pwr,
v = NULL,
f2 = f2_to_calculate_n,  # Set to NULL to solve for effect size
sig.level = alpha.pwr,
power = power.pwr
)
print(sensitivity_analysis_to_calculate_n)
n_to_calcualte_n = ceiling(sensitivity_analysis_to_calculate_n$v + k.pwr + 1)
# lets create a plot to see what is the variance in the data
data_standardized <- as.data.frame(scale(data_numeric))
calculate_cv <- function(x) {
(sd(x) / abs(mean(x))) * 100
}
cv_results <- sapply(data_numeric, calculate_cv)
cv_results_decreasing_order <- order(cv_results, decreasing = T)
data_standardized_order <- data_standardized[, cv_results_decreasing_order]
cv_results_order <- cv_results[cv_results_decreasing_order]
mgp.current = par("mgp")
png(filename = paste0("plots/z_score_data_numeric.png"), width = 1096, height = 555, units = "px")
par(mar = c(11, 2.8, 4.5, 0.1), mgp = c(1.5, 0.5, 0))
bp = boxplot(
data_standardized_order,
ylab = "Standardized Value (Z-Score)",
col = "lightblue",
cex.axis = 1.3,
cex.lab = 1.3,
cex = 1.3,
las = 3, outpch = 16, outcol = "red",
names = FALSE, xaxt = "n",
boxwex = 0.6,
xlim = c(3, ncol(data_standardized_order) -2)
)
grid(nx = 25, ny = NULL, lty = 2)
axis(1, at = 1:ncol(data_standardized_order),
labels = parse(text = names_data_numeric[cv_results_decreasing_order]),
las = 2,
cex.axis = 1.3)
ymax.bp = max(bp$stats[5,], bp$out)
ymin.bp = min(bp$stats[1,], bp$out)
yrange.bp = ymax.bp - ymin.bp
text(x = 1:length(bp$n), y = ymax.bp + 0.25*yrange.bp,
labels = paste0(round(cv_results_order,2),"%"), cex = 1.3, xpd = TRUE,
srt = 90, adj = 1)
dev.off()
par(mgp = mgp.current)
# adding the best prediction to the cities and saving them as CSV
cities_RO <- cities_RO %>%
st_join(
RO_global_horizontal_data %>%
dplyr::select(final_yield_estimated, final_yield_lower_95_ci, final_yield_upper_95_ci,
final_yield_lower_95_pi, final_yield_upper_95_pi, CUF_over_ISF),
join = st_nearest_feature,
left = TRUE
)
cities_RO_with_prediction <- cities_RO %>%
mutate(
final_yield_estimated = as.numeric(final_yield_estimated),
final_yield_lower_95_ci = as.numeric(final_yield_lower_95_ci),
final_yield_upper_95_ci = as.numeric(final_yield_upper_95_ci),
final_yield_lower_95_pi = as.numeric(final_yield_lower_95_pi),
final_yield_upper_95_pi = as.numeric(final_yield_upper_95_pi),
CUF_over_ISF = as.numeric(CUF_over_ISF),
Longitude = st_coordinates(.)[, "X"],
Latitude = st_coordinates(.)[, "Y"]
) %>%
st_drop_geometry() %>%
filter(!is.na(final_yield_estimated)) %>%
dplyr::select(nome, geocodigo, Longitude, Latitude, Samples,
Samples_clean, final_yield_estimated,
final_yield_lower_95_ci, final_yield_upper_95_ci,
final_yield_lower_95_pi, final_yield_upper_95_pi)
write_csv(cities_RO_with_prediction, "cities_RO_final_yield_prediction.csv")
# Now join this non-spatial data frame to the spatial municipality map (map_RO)
map_RO <- map_RO %>%
left_join(cities_RO %>% st_drop_geometry(), by = "geocodigo")
plot_map_RO(map_RO, "final_yield_estimated",
map_legend_title = "Final yield (kWh/kWp)",
"final_yield_cities_fit_RO")
plot_map_RO(map_RO, "final_yield_lower_95_ci",
map_legend_title = "Final yield lower 95% confidence interval (kWh/kWp)",
"final_yield_cities_lower_95_ci_RO")
plot_map_RO(map_RO, "final_yield_upper_95_ci",
map_legend_title = "Final yield upper 95% confidence interval (kWh/kWp)",
"final_yield_cities_upper_95_ci_RO")
plot_map_RO(map_RO, "final_yield_lower_95_pi",
map_legend_title = "Final yield lower 95% prediction interval (kWh/kWp)",
"final_yield_cities_lower_95_pi_RO")
plot_map_RO(map_RO, "final_yield_upper_95_pi",
map_legend_title = "Final yield upper 95% prediction interval (kWh/kWp)",
"final_yield_cities_upper_95_pi_RO")
plot_map_RO(map_RO, "CUF_over_ISF",
map_legend_title = "capacity utilization factor over inverter sizing factor (%)",
"CUF_over_ISF_cities_fit_RO")
############## Now, we will try something different. We will fit a GAM with all variables and see what happens
# in case we need to load the data
# data_cleanest = read.csv("data_cleanest.csv")
# we will not use these variables, because they are either results, repeated
# or only have one identical datum
not_to_fit_columns = c("PR_GHI", "TOTAL_kWh", "final_yield",
"hours_peak_sun", "CUF", "final_yield_estimated",
"outliers", "geocodigo", "monocristalline")
# our categorical variables
categorical_predictors = c("MOD_MAKER", "MOD_MODEL", "INV_MAKER",
"INV_MODEL", "CITY_IBGE", "geocodigo",
"p_type", "monofacial")
# these are all the columns in the data
all_names = names(data_cleanest)
# we are assuming that numerical variables are all that are not the two described
# above
candidate_numerical_predictors <- all_names[
!(all_names %in% c(not_to_fit_columns, categorical_predictors))
]
# now, we will only use variables that have at least n distinctive values
# this is for cleaning and allowing GAM to converge
n_distinct_check <- data.frame(
variable = candidate_numerical_predictors,
n_distinct = sapply(candidate_numerical_predictors, function(var_name) {
return(dplyr::n_distinct(data_cleanest[[var_name]], na.rm = TRUE))
})
)
final_continuous_predictors <- n_distinct_check %>%
dplyr::filter(n_distinct >= 10) %>%
pull(variable)
# now we write the formula
numerical_terms_in_GAM <- paste0("s(", final_continuous_predictors, ", bs='ts')", collapse = " + ")
categorical_terms_in_GAM <- paste0("as.factor(", categorical_predictors, ")", collapse = " + ")
formula_gam_total = as.formula(paste("PR_GHI ~", categorical_terms_in_GAM, "+", numerical_terms_in_GAM))
gam_penalizad_total <- gam(
formula_gam_total,
data = data_cleanest,
method = "REML",
select = TRUE
) # this can take a while
print(summary(gam_penalizad_total))
# R2 is negative and Deviance is large. We have overfitting. Note that almost
# all s() have edf close to zero
# so, GAM tells us a lot of the variables cannot explain PR_GHI. Lets see if other
# method like Random Forest can
library(randomForest)
library(pdp)
library(caret)
library(e1071)
set.seed(42)
collumns_for_rf_total <- c("PR_GHI", final_continuous_predictors, categorical_predictors)
# Lets create a dataframe for random forests
rf_data_brute_force <- data_cleanest %>%
dplyr::select(all_of(collumns_for_rf_total)) %>%
mutate(across(all_of(categorical_predictors), as.factor))
n_predictors <- ncol(rf_data_brute_force) - 1 # number of predictors
# lets see what happens with different variables to try at each split
mtry_grid <- expand.grid(
.mtry = unique(c(1, 2, 5, 10, round(n_predictors / 3), round(n_predictors / 2), n_predictors - 5))
)
# lets define a 5-fold cross-validation repeated 3 times
fit_control <- trainControl(
method = "repeatedcv",
number = 5,
repeats = 3,
verboseIter = FALSE
)
# lets train
rf_tuned <- train(
PR_GHI ~ .,
data = rf_data_brute_force,
method = "rf",
metric = "Rsquared",
tuneGrid = mtry_grid,
trControl = fit_control,
importance = TRUE
)
print(rf_tuned)
plot(rf_tuned)
######### Now, lets run recursive variable elimination
subset_sizes <- c(1, 2, 5, 10, 20, 30)
# lets define a 5-fold cross-validation repeated 3 times
rfe_control <- rfeControl(
functions = rfFuncs,
method = "repeatedcv",
number = 5,
repeats = 3,
verbose = TRUE
)
# lets train
rfe_results <- rfe(
PR_GHI ~ .,
data = rf_data_brute_force,
sizes = subset_sizes,
rfeControl = rfe_control,
metric = "Rsquared"
)
print(rfe_results)
plot(rfe_results)
